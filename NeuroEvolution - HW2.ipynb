{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce12136",
   "metadata": {},
   "source": [
    "# HW2 - DENSER-Like Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f44c8b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae5d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import warnings \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras import datasets, Sequential, layers, models, initializers, regularizers, optimizers, metrics, utils\n",
    "from tensorflow.keras import models\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from PIL import ImageFile\n",
    "import tensorflow.keras.utils  as utils\n",
    "from IPython.display import display\n",
    "import pydot\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673cfd58",
   "metadata": {},
   "source": [
    "## Definition of the string-based genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eadddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Linear', [('number_of_features', 268), ('bias', 0.737495)])\n",
      "('BatchNorm1d', [('num_features', 227), ('eps', 0.000529), ('momentum', 0.495779)])\n",
      "('LayerNorm', [('normalized_shape', 100), ('eps', 0.006907)])\n",
      "('Dropout', [('p', 0.115773)])\n",
      "('AlphaDropout', [('p', 0.704755)])\n",
      "('Activation', [('function', 'SiLU')])\n"
     ]
    }
   ],
   "source": [
    "#Defining a string-based representation for the networks (the genotype)\n",
    "layers_and_activations = [\n",
    "    ('Linear', [('number_of_features', 1, 512), ('bias', 0.0, 1.0)]),\n",
    "    ('BatchNorm1d', [('num_features', 1, 512),('eps', 1e-5, 1e-2), ('momentum', 0.1, 0.99)]),\n",
    "    ('LayerNorm', [('normalized_shape', 1, 512), ('eps', 1e-5, 1e-2)]),\n",
    "    ('Dropout', [('p', 0.0, 1.0)]),\n",
    "    ('AlphaDropout', [('p', 0.0, 1.0)]),\n",
    "    ('Activation', ['Sigmoid', 'ReLU', 'PReLU', 'ELU', 'SELU', 'GELU', 'CELU', 'SiLU']),\n",
    "]\n",
    "\n",
    "\n",
    "def select_random_params(layer_tuple):\n",
    "    \"\"\"\n",
    "    Selects random parameters within the provided range for a given layer.\n",
    "    \"\"\"\n",
    "    layer_name, param_list = layer_tuple\n",
    "    random_params = []\n",
    "\n",
    "    # For 'Activation', we just want to randomly select one from the list\n",
    "    if layer_name == 'Activation':\n",
    "        random_params.append(('function', random.choice(param_list)))\n",
    "    else:\n",
    "        for param_tuple in param_list:\n",
    "            if isinstance(param_tuple[1], int):  # integer parameter\n",
    "                random_value = random.randint(param_tuple[1], param_tuple[2])\n",
    "            else:  # float parameter\n",
    "                random_value = random.uniform(param_tuple[1], param_tuple[2])\n",
    "                random_value = round(random_value, 6)  # rounding to keep the number manageable\n",
    "\n",
    "            random_params.append((param_tuple[0], random_value))\n",
    "\n",
    "    return layer_name, random_params\n",
    "\n",
    "# Test the function with each layer\n",
    "for layer in layers_and_activations:\n",
    "    print(f\"{select_random_params(layer)}\")\n",
    "\n",
    "\n",
    "def generate_random_model_spec(input_features, output_features):\n",
    "    model_spec = []\n",
    "\n",
    "    # Input layer\n",
    "    layer_name, layer_params = select_random_params(layers_and_activations[0])\n",
    "    layer_params_dict = dict(layer_params)  # Convert the list of tuples to a dictionary\n",
    "    last_out_features = layer_params_dict['number_of_features']\n",
    "    model_spec.append(('Linear', {'in_features': input_features, 'out_features': last_out_features}))\n",
    "\n",
    "    # Hidden layers\n",
    "    for i in range(random.randint(1, 5)):  # Choose a random number of layers to add\n",
    "        layer_choice = random.choice(layers_and_activations)\n",
    "        layer_name, layer_params = select_random_params(layer_choice)\n",
    "        layer_params_dict = dict(layer_params)  # Convert the list of tuples to a dictionary\n",
    "        if layer_name == 'Linear':\n",
    "            last_out_features = layer_params_dict['number_of_features']\n",
    "            layer_params_dict = {'in_features': last_out_features, 'out_features': layer_params_dict['number_of_features']}\n",
    "        elif layer_name in ['BatchNorm1d']:\n",
    "            layer_params_dict = {'num_features': last_out_features}\n",
    "        elif layer_name in ['LayerNorm']:\n",
    "            layer_params_dict = {'normalized_shape': last_out_features}\n",
    "        model_spec.append((layer_name, layer_params_dict))\n",
    "\n",
    "    # Output layer\n",
    "    model_spec.append(('Linear', {'in_features': last_out_features, 'out_features': output_features}))\n",
    "\n",
    "    return model_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f5c65",
   "metadata": {},
   "source": [
    "## Defining a network class that is able to parse those instructions and build a functional pytorch network structure (the phenotype)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a99de97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Test the functions\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m random_model_spec \u001b[38;5;241m=\u001b[39m generate_random_model_spec(\u001b[43minput_size\u001b[49m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(random_model_spec)\n\u001b[0;32m     43\u001b[0m random_model \u001b[38;5;241m=\u001b[39m create_model_from_spec(random_model_spec)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_size' is not defined"
     ]
    }
   ],
   "source": [
    "#Each network needs to have at least 1 layer and a maximum 50 layers. This choice\n",
    "#needs to be made at random when generating the genotype.\n",
    "\n",
    "\n",
    "class DeepNetwork:\n",
    "    def __init__(self, genotype, phenotype):\n",
    "        self.genotype\n",
    "        self.phenotype\n",
    "    \n",
    "    def set_genotype(self, genotype_value):\n",
    "        self.genotype = genotype_value\n",
    "\n",
    "    def set_phenotype(self, phenotype_value):\n",
    "        self.phenotype = phenotype_value\n",
    "\n",
    "    def genotype_to_phenotype(genotype):\n",
    "        #Tenho de construir aqui o transformador de g para f        \n",
    "        self.genotype = genotype_value\n",
    "        return genotype\n",
    "    \n",
    "    \n",
    "import random\n",
    "\n",
    "\n",
    "def create_model_from_spec(model_spec):\n",
    "    model = nn.Sequential()\n",
    "\n",
    "    for i, (layer_name, layer_params) in enumerate(model_spec):\n",
    "        if layer_name == 'Activation':\n",
    "            activation_function = getattr(nn, layer_params['function'])()\n",
    "            model.add_module(f'Activation{i}', activation_function)\n",
    "        else:\n",
    "            layer = getattr(nn, layer_name)(**layer_params)\n",
    "            model.add_module(f'{layer_name}{i}', layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "random_model_spec = generate_random_model_spec(input_size, 10)\n",
    "print(random_model_spec)\n",
    "\n",
    "random_model = create_model_from_spec(random_model_spec)\n",
    "print(random_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae1d35",
   "metadata": {},
   "source": [
    "## Defining a string-based representation for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888da8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a string-based representation for the networks (the genotype)\n",
    "optimizers = [\n",
    "    ('Adam', [('lr', -3, 0), ('beta1', 0.8, 0.9), ('beta2', 0.99, 0.999)]),\n",
    "    ('AdamW', [('lr', -4, -1), ('beta1', 0.8, 0.9), ('beta2', 0.99, 0.999), ('weight_decay', -4, -1)]),\n",
    "    ('Adadelta', [('lr', -3, 0), ('rho', 0.9, 0.99)]),\n",
    "    ('NAdam', [('lr', -3, 0), ('beta1', 0.8, 0.9), ('beta2', 0.99, 0.999), ('momentum_decay', 0.003, 0.005)]),\n",
    "    ('SGD', [('lr', -3, 0), ('momentum', 0.5, 0.99), ('nesterov', False, True)]),\n",
    "]\n",
    "\n",
    "def select_random_params_op(optimizer_tuple):\n",
    "    \"\"\"\n",
    "    Selects random parameters within the provided range for a given optimizer.\n",
    "    \"\"\"\n",
    "    optimizer_name, param_list = optimizer_tuple\n",
    "    random_params = []\n",
    "\n",
    "    for param_tuple in param_list:\n",
    "        if param_tuple[0] == 'nesterov':  # boolean parameter\n",
    "            random_value = random.choice([param_tuple[1], param_tuple[2]])\n",
    "        else:  # float parameter\n",
    "            random_value = 10 ** random.uniform(param_tuple[1], param_tuple[2])\n",
    "            random_value = round(random_value, 6)  # rounding to keep the number manageable\n",
    "\n",
    "        random_params.append((param_tuple[0], random_value))\n",
    "\n",
    "    return optimizer_name, random_params\n",
    "\n",
    "# Test the function with each optimizer\n",
    "for optimizer in optimizers:\n",
    "    print(f\"{select_random_params(optimizer)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211dd29",
   "metadata": {},
   "source": [
    "## Defining a way to parse those instructions and build and functional pytorch optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf73e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepOptimizers:\n",
    "    def __init__(self, genotype=None, phenotype=None):\n",
    "        if genotype is None:\n",
    "            self.genotype = genotype\n",
    "            self.phenotype = phenotype\n",
    "        else:\n",
    "            self.genotype = genotype\n",
    "            self.phenotype = phenotype\n",
    "    \n",
    "    def set_genotype(self, genotype_value):\n",
    "        self.genotype = genotype_value\n",
    "\n",
    "    def set_phenotype(self, phenotype_value):\n",
    "        self.phenotype = phenotype_value\n",
    "\n",
    "    def genotype_to_phenotype(genotype):\n",
    "        #Tenho de construir aqui o transformador de g para f        \n",
    "        self.genotype = genotype_value\n",
    "        return genotype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64018b",
   "metadata": {},
   "source": [
    "## Sample all parameter values from a grammar (this allows for a restricted search space and removes the need to deal with invalid combinations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc94020",
   "metadata": {},
   "source": [
    "### Rules\n",
    "1. BatchNorm vs LayerNorm\n",
    "2. Dropout vs AlphaDropout\n",
    "3. Dropout or Norm after Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7525ce",
   "metadata": {},
   "source": [
    "## Define 4 simple genetic operators:\n",
    "1. Network crossover\n",
    "2. Add layer mutation\n",
    "3. Remove layer mutation\n",
    "4. Change optimizer mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d91d04",
   "metadata": {},
   "source": [
    "## Experimental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ce82c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_population(genotype=None):\n",
    "    return ([DeepNetwork(genotype=genotype) for _ in range(5)])\n",
    "\n",
    "def run(self, selection=0, crossover=0, mutation=0, metric=\"manhattan\"):\n",
    "    self.initialize_population(policy_matrix=None)\n",
    "\n",
    "    for gen in range(self.generations):\n",
    "        for agent in self.population:\n",
    "            self.evaluate_fitness(agent, metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fef7d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (InputLayer): Linear(in_features=784, out_features=248, bias=True)\n",
      "  (InputActivation): ReLU()\n",
      "  (HiddenLinear5): Linear(in_features=248, out_features=63, bias=True)\n",
      "  (HiddenActivation5): ReLU()\n",
      "  (OutputLayer): Linear(in_features=63, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def create_random_model(input_features, output_features):\n",
    "    model = nn.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    layer_name, layer_params = select_random_params(('Linear', [('number_of_features', 1, 512), ('bias', 0.0, 1.0)]))\n",
    "    layer_params_dict = dict(layer_params)  # Convert the list of tuples to a dictionary\n",
    "    last_out_features = layer_params_dict['number_of_features']\n",
    "    model.add_module('InputLayer', nn.Linear(input_features, last_out_features))\n",
    "    model.add_module('InputActivation', nn.ReLU())  # Adding an activation function\n",
    "\n",
    "    # Hidden layers\n",
    "    for i in range(random.randint(1, 50)):  # Choose a random number of layers to add\n",
    "        layer_choice = random.choice(layers_and_activations)\n",
    "        if layer_choice[0] != 'Linear':  # We don't want to start a hidden layer with anything other than a Linear layer\n",
    "            continue\n",
    "        layer_name, layer_params = select_random_params(layer_choice)\n",
    "        layer_params_dict = dict(layer_params)  # Convert the list of tuples to a dictionary\n",
    "        model.add_module('HiddenLinear' + str(i), nn.Linear(last_out_features, layer_params_dict['number_of_features']))\n",
    "        last_out_features = layer_params_dict['number_of_features']  # Update the last output features count\n",
    "        model.add_module('HiddenActivation' + str(i), nn.ReLU())  # Adding an activation function\n",
    "        \n",
    "        # Optionally add a dropout or normalization layer\n",
    "        if random.choice([True, False]):\n",
    "            layer_choice = random.choice(layers_and_activations)\n",
    "            if layer_choice[0] in ['Dropout', 'AlphaDropout']:\n",
    "                layer_name, layer_params = select_random_params(layer_choice)\n",
    "                layer_params_dict = dict(layer_params)\n",
    "                model.add_module('Hidden' + layer_name + str(i), getattr(nn, layer_name)(**layer_params_dict))\n",
    "            elif layer_choice[0] in ['BatchNorm1d','LayerNorm']:\n",
    "                layer_name, layer_params = select_random_params(layer_choice)\n",
    "                layer_params_dict = dict(layer_params)\n",
    "                model.add_module('Hidden' + layer_name + str(i), getattr(nn, layer_name)(last_out_features, **layer_params_dict))\n",
    "\n",
    "    # Output layer\n",
    "    model.add_module('OutputLayer', nn.Linear(last_out_features, output_features))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Test the function\n",
    "random_model = create_random_model(input_size, 10)\n",
    "print(random_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f97ad8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_optimizer(model_parameters):\n",
    "    # Choose a random optimizer\n",
    "    optimizer_choice = random.choice(optimizers)\n",
    "\n",
    "    # Get random parameters for the chosen optimizer\n",
    "    optimizer_name, optimizer_params = select_random_params_op(optimizer_choice)\n",
    "    optimizer_params_dict = dict(optimizer_params)  # Convert the list of tuples to a dictionary\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = getattr(torch.optim, optimizer_name)(model_parameters, **optimizer_params_dict)\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa157353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.024454\n",
      "    maximize: False\n",
      "    momentum: 4.342836\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "random_optimizer = create_random_optimizer(random_model.parameters())\n",
    "print(random_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e285e4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Accuracy 0.1063\n",
      "Epoch 1  Accuracy 0.1057\n",
      "Epoch 2  Accuracy 0.1046\n",
      "Epoch 3  Accuracy 0.1075\n",
      "Epoch 4  Accuracy 0.1041\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    accuracy_hist_train = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.view(x_batch.size(0), -1)  # flatten the input\n",
    "        pred = random_model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "        random_optimizer.step()\n",
    "        random_optimizer.zero_grad()\n",
    "        is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "        accuracy_hist_train += is_correct.sum()\n",
    "    accuracy_hist_train /= len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch}  Accuracy {accuracy_hist_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cd599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
